{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4a1d17-4491-4f5e-8560-1f345412ef5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This Nb is one of the Tasks of the 251201_job_demo Job.\n",
    "# The Job has 2 parameters and is configured to be triggered on file arrival to the location below.\n",
    "# This Nb will get executed as soon as a new file arrives to the location below.\n",
    "my_catalog = dbutils.widgets.get(\"catalog\")\n",
    "my_schema = dbutils.widgets.get(\"schema\")\n",
    "my_volume_path = f\"/Volumes/{my_catalog}/{my_schema}/dataset/trigger_storage_location/\"\n",
    "\n",
    "print (my_volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6f67a2-ec28-4125-9bb4-1d75308dde21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "#\n",
    "# Create customers table from customers.csv\n",
    "#\n",
    "\n",
    "# Step 1: Read csv file into a data frame\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(my_volume_path)\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    "    .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "#display(df)\n",
    "\n",
    "# Step 2: Create a table \n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{my_catalog}.{my_schema}.bronze_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1865c37-c85b-4546-9b80-7eb80549ca65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Post-check cell. Do not include it later in the job\n",
    "spark.read.table(\"job_catalog.default.bronze_customers\").display()\n",
    "# OBS! the spark.read.table() reads a table from the catalog and saves it in a data frame"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8075692741621517,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_customers_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
