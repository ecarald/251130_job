{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a36c1f1-cb43-43b1-910d-bb69dba463a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Pre-check to inspect the csv file. Do not include it in the job.\n",
    "/*\n",
    "select * \n",
    "from read_files(\n",
    "  '/Volumes/job_catalog/default/dataset/orders/',\n",
    "  format => 'csv'\n",
    "  );\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c765c828-6b4a-4ce8-917c-bc8ba0aeaa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, current_timestamp, col\n",
    "\n",
    "#Ingest data from the orders.csv file and create the bronze_orders table\n",
    "\n",
    "#Step 1: Read the csv file and create a Spark DataFrame\n",
    "file_path = \"/Volumes/job_catalog/default/dataset/orders/\"\n",
    "csv_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('customer_name', StringType(), True),\n",
    "    StructField('product_name', StringType(), True),\n",
    "    StructField('order_date', DateType(), True),\n",
    "    StructField('product_category', StringType(), True),\n",
    "    StructField('total_price', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_orders = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(csv_schema)\n",
    "    .load(file_path)\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    "    .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "#Step 2: Create a delta table\n",
    "(df_orders\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".format(\"delta\")\n",
    ".saveAsTable(\"job_catalog.default.bronze_orders\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec03218-d778-4db2-ba63-2f1bc295df69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Post-check. Do not include in the job\n",
    "#spark.read.table(\"job_catalog.default.bronze_orders\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5469251483787007,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_orders_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
